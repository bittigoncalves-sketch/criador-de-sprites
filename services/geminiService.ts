import { GoogleGenAI, Modality, GenerateContentResponse, Content } from "@google/genai";
import type { ChatMessage } from "../types";

// Creates a new AI client for each call to ensure the most recent API key is used,
// especially important in environments where the key can be selected by the user at runtime.
const getAiClient = () => {
    const apiKey = process.env.API_KEY;
    if (!apiKey) {
        console.error("API_KEY is not available. Please ensure it is set in the environment.");
        // We throw here to prevent API calls with an undefined key.
        // The UI should handle this gracefully, e.g., by prompting for a key.
        throw new Error("API Key not found.");
    }
    return new GoogleGenAI({ apiKey });
};


const fileToGenerativePart = async (file: File) => {
  const base64EncodedDataPromise = new Promise<string>((resolve) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(file);
  });
  return {
    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
  };
};

export const generateImageFromText = async (prompt: string, aspectRatio: string): Promise<string> => {
    const ai = getAiClient();
    const response = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt,
        config: {
            numberOfImages: 1,
            outputMimeType: 'image/jpeg',
            aspectRatio: aspectRatio as "1:1" | "3:4" | "4:3" | "9:16" | "16:9",
        },
    });

    const base64ImageBytes = response.generatedImages[0].image.imageBytes;
    return `data:image/jpeg;base64,${base64ImageBytes}`;
};


export const analyzeImageContent = async (imageFile: File, prompt: string): Promise<string> => {
    const ai = getAiClient();
    const imagePart = await fileToGenerativePart(imageFile);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: { parts: [imagePart, { text: prompt }] },
    });
    return response.text;
};

export const editImageWithPrompt = async (imageFile: File, prompt: string): Promise<string> => {
    const ai = getAiClient();
    const imagePart = await fileToGenerativePart(imageFile);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [imagePart, { text: prompt }],
        },
        config: {
            responseModalities: [Modality.IMAGE],
        },
    });

    for (const part of response.candidates?.[0]?.content.parts ?? []) {
        if (part.inlineData) {
            const base64ImageBytes: string = part.inlineData.data;
            return `data:${part.inlineData.mimeType};base64,${base64ImageBytes}`;
        }
    }
    throw new Error('No image generated by the model.');
};


export const generateSpriteSheet = async (referenceFile: File, animationPrompt: string, frameCount: number = 4): Promise<string[]> => {
    const results: string[] = [];
    for (let i = 1; i <= frameCount; i++) {
        const prompt = `Based on the reference character, create frame ${i} of a ${frameCount}-frame animation for "${animationPrompt}". Isolate the character on a transparent background.`;
        const imageUrl = await editImageWithPrompt(referenceFile, prompt);
        results.push(imageUrl);
    }
    return results;
};


export const generateVideoFromImage = async (imageFile: File, prompt: string, aspectRatio: '16:9' | '9:16') => {
    const ai = getAiClient();
    const base64EncodedDataPromise = new Promise<string>((resolve) => {
        const reader = new FileReader();
        reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
        reader.readAsDataURL(imageFile);
    });
    const base64Data = await base64EncodedDataPromise;

    let operation = await ai.models.generateVideos({
        model: 'veo-3.1-fast-generate-preview',
        prompt,
        image: {
            imageBytes: base64Data,
            mimeType: imageFile.type,
        },
        config: {
            numberOfVideos: 1,
            resolution: '720p',
            aspectRatio: aspectRatio
        }
    });
    return operation;
};

export const checkVideoOperationStatus = async (operation: any) => {
    const ai = getAiClient();
    return await ai.operations.getVideosOperation({ operation });
};


// This stateless function sends the entire chat history for context, ensuring that
// conversation memory is maintained even when switching modes (search, thinking).
export const sendMessageToChat = async (
    history: ChatMessage[],
    newMessage: string,
    useSearch: boolean,
    useThinking: boolean
): Promise<GenerateContentResponse> => {
    const ai = getAiClient();

    // Convert our app's message format to the Gemini API's format.
    const contents: Content[] = history.map(msg => ({
        role: msg.role,
        parts: [{ text: msg.text }]
    }));
    contents.push({ role: 'user', parts: [{ text: newMessage }] });


    let model = 'gemini-2.5-flash';
    let config: any = {};

    if (useThinking) {
        model = 'gemini-2.5-pro';
        config.thinkingConfig = { thinkingBudget: 32768 };
    }
    
    if (useSearch) {
        model = "gemini-2.5-flash";
        config.tools = [{googleSearch: {}}];
    }
    
    return ai.models.generateContent({
        model: model,
        contents: contents,
        config: config
    });
};
