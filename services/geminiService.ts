
import { GoogleGenAI, Modality, GenerateContentResponse, Content } from "@google/genai";
import type { ChatMessage } from "../types";

// Retrieves the AI client, using the API key from the user's browser localStorage.
const getAiClient = async () => {
    // For Video Generation (Veo), a special key selected in the UI is used via process.env.
    // For all other models, we use the key from localStorage.
    const apiKey = localStorage.getItem('gemini_api_key');

    if (!apiKey) {
        console.error("API Key not found in localStorage.");
        throw new Error("API Key not set. Please add your key in the settings.");
    }
    return new GoogleGenAI({ apiKey });
};

const getVeoAiClient = async () => {
    // Veo uses a specific key selection mechanism provided by the environment.
    const apiKey = process.env.API_KEY;
    if (!apiKey) {
        console.error("Veo API Key is not available.");
        throw new Error("API Key for video generation not found. Please select a key in the Video Gen tab.");
    }
    return new GoogleGenAI({ apiKey });
}

const fileToImagePart = async (file: File) => {
  const base64EncodedData = await new Promise<string>((resolve) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(file);
  });
  return {
    imageBytes: base64EncodedData,
    mimeType: file.type,
  };
};


const fileToGenerativePart = async (file: File) => {
  const base64EncodedDataPromise = new Promise<string>((resolve) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(file);
  });
  return {
    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
  };
};

export const generateImageFromText = async (prompt: string, aspectRatio: string): Promise<string> => {
    const ai = await getAiClient();
    const response = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt,
        config: {
            numberOfImages: 1,
            outputMimeType: 'image/jpeg',
            aspectRatio: aspectRatio as "1:1" | "3:4" | "4:3" | "9:16" | "16:9",
        },
    });

    const base64ImageBytes = response.generatedImages[0].image.imageBytes;
    return `data:image/jpeg;base64,${base64ImageBytes}`;
};


export const analyzeImageContent = async (imageFile: File, prompt: string): Promise<string> => {
    const ai = await getAiClient();
    const imagePart = await fileToGenerativePart(imageFile);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: { parts: [imagePart, { text: prompt }] },
    });
    return response.text;
};

export const editImageWithPrompt = async (imageFile: File, prompt: string): Promise<string> => {
    const ai = await getAiClient();
    const imagePart = await fileToGenerativePart(imageFile);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [imagePart, { text: prompt }],
        },
        config: {
            responseModalities: [Modality.IMAGE],
        },
    });

    for (const part of response.candidates?.[0]?.content.parts ?? []) {
        if (part.inlineData) {
            const base64ImageBytes: string = part.inlineData.data;
            return `data:${part.inlineData.mimeType};base64,${base64ImageBytes}`;
        }
    }
    throw new Error('No image generated by the model.');
};


export const generateSpriteSheet = async (referenceFile: File, animationPrompt: string, frameCount: number = 4): Promise<string[]> => {
    const results: string[] = [];
    for (let i = 1; i <= frameCount; i++) {
        const prompt = `Based on the reference character, create frame ${i} of a ${frameCount}-frame animation for "${animationPrompt}". Isolate the character on a transparent background.`;
        const imageUrl = await editImageWithPrompt(referenceFile, prompt);
        results.push(imageUrl);
    }
    return results;
};

export const sendMessageToChat = async (
    history: ChatMessage[],
    newMessage: string,
    useSearch: boolean,
    useThinking: boolean
): Promise<GenerateContentResponse> => {
    const ai = await getAiClient();

    const contents: Content[] = history.map(msg => ({
        role: msg.role,
        parts: [{ text: msg.text }]
    }));
    contents.push({ role: 'user', parts: [{ text: newMessage }] });


    let model = 'gemini-2.5-flash';
    let config: any = {};

    if (useThinking) {
        model = 'gemini-2.5-pro';
        config.thinkingConfig = { thinkingBudget: 32768 };
    }
    
    if (useSearch) {
        model = "gemini-2.5-flash";
        config.tools = [{googleSearch: {}}];
    }
    
    return ai.models.generateContent({
        model: model,
        contents: contents,
        config: config
    });
};

export const generateVideoFromImage = async (imageFile: File, prompt: string, aspectRatio: '16:9' | '9:16') => {
    const ai = await getVeoAiClient(); // Use Veo-specific client
    const image = await fileToImagePart(imageFile);
    const operation = await ai.models.generateVideos({
      model: 'veo-3.1-fast-generate-preview',
      prompt: prompt,
      image: image,
      config: {
        numberOfVideos: 1,
        resolution: '720p',
        aspectRatio: aspectRatio
      }
    });
    return operation;
};

export const checkVideoOperationStatus = async (operation: any) => {
    const ai = await getVeoAiClient(); // Use Veo-specific client
    return await ai.operations.getVideosOperation({ operation: operation });
};
